<!DOCTYPE html>
<html>
<!-- <link href="https://fonts.cdnfonts.com/css/caveat" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Handlee&v1" rel="stylesheet">

<style>
    @import url('https://fonts.cdnfonts.com/css/caveat');
</style> -->
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SkyActor</title>
  <link rel="icon" type="image/x-icon" href="static/images/logo.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>










  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="static/images/logo.png" style="height:256px; margin-bottom: -10px;"></img>
            <h1 class="title is-3 publication-title">SkyActor: Generative Expressive Character</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=F4lns_oAAAAJ&hl" target="_blank">Tao Wu</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://yzhang2016.github.io/" target="_blank">Yong Zhang</a><sup>2†</sup>,</span>
                  <span class="author-block">
                    <a href="https://xinntao.github.io/" target="_blank">Xintao Wang</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=N8mK__gAAAAJ" target="_blank">Xinpan Zhou</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=i1PqK7kAAAAJ" target="_blank">Guangcong Zheng</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=zJvrrusAAAAJ" target="_blank">Zhongang Qi</a><sup>3</sup>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ" target="_blank">Ying Shan</a><sup>2,3</sup>
                  </span>
                  <span class="author-block">
                    <a href="https://person.zju.edu.cn/xilics" target="_blank">Xi Li</a><sup>1†</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Zhejiang University</span>&nbsp;&nbsp;
                    <span class="author-block"><sup>2</sup>Tencent AI Lab</span>&nbsp;&nbsp;
                    <span class="author-block"><sup>3</sup>ARC Lab, Tencent PCG</span>
                  </div>
                  
                  <br>


              


          


                    


                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                    <span class="link-block">
                        <a href="https://arxiv.org/abs/2408.13239" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>
                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="./Appendix.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/WuTao-CS/CustomCrafter"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                      </a>

                  </span>

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/omerbt/TokenFlow" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
              <!-- Hugging Face Demo link with an image icon -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/spaces/weizmannscience/tokenflow" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="static/images/hf.png" alt="Hugging Face Demo">
                  </span>
                  <span>Demo</span>
                </a>
              </span> -->
                <!-- ArXiv abstract Link -->
                
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Teaser video-->
<section class="hero teaser is-light" align="center">
  <div class="container is-max-desktop">
    <br>
    <h2 class="title is-3">Overview of Comparison and Generated Videos</h2>
    
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/demo.mp4"
        type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2> -->
    </div>
  </div>
</section>

<!-- End teaser video -->

<!-- Paper abstract -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Customized video generation aims to generate high-quality videos guided by text prompts and subject's reference images. However, since it is only trained on static images, the fine-tuning process of subject learning disrupts abilities of video diffusion models (VDMs) to combine concepts and generate motions. To restore these abilities, some methods use additional video similar to the prompt to fine-tune or guide the model. This requires frequent changes of guiding videos and even re-tuning of the model when generating different motions, which is very inconvenient for users. In this paper, we propose CustomCrafter, a novel framework that preserves the model's motion generation and conceptual combination abilities without additional video and fine-tuning to recovery. For preserving conceptual combination ability, we design a plug-and-play module to update few parameters in VDMs, enhancing the model's ability to capture the appearance details and the ability of concept combinations for new subjects. For motion generation, we observed that VDMs tend to restore the motion of video in the early stage of denoising, while focusing on the recovery of subject details in the later stage. Therefore, we propose Dynamic Weighted Video Sampling Strategy. Using the pluggability of our subject learning modules, we reduce the impact of this module on motion generation in the early stage of denoising, preserving the ability to generate motion of VDMs. In the later stage of denoising, we restore this module to repair the appearance details of the specified subject, thereby ensuring the fidelity of the subject's appearance. Experimental results show that our method has a significant improvement compared to previous methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
<!--         
      <table width="800" border="0">
        <tbody>
          <tr>
            <td colspan="3">
              <p>
                We observe that the level of temporal consistency of a video is tightly related to the temporal consistency of its feature representation, as can be seen in the feature visualization below.
                The features of a natural video have a shared, temporally consistent representation. When editing the video per frame, this consistency breaks. Our method ensures the same level of feature consistency as in the original video features.
              </p>
            </td>
          </tr>
          <tr>
            <td style="font-size: 16px; text-align: center;">Original</td>
            <td style="font-size: 16px; text-align: center;">Per Frame Editing</td>
            <td style="font-size: 16px; text-align: center;">Ours</td>
          </tr>
          <tr class="video-row">
            <td style="text-align: center;">
              <a href="sm/assets/man_basket/input_fps30.mp4">
                <video preload="auto"width="224" src="sm/assets/man_basket/input_fps30.mp4" autoplay loop controls muted/>
              </a>
            </td>
            <td style="text-align: center;">
              <a href="sm/assets/man_basket/Van-Gogh style portrait of a man spinning a basketball, oil painting, art by Van Gogh, 8k/pnp_per_frame_baseline_fps_30.mp4">
                <video preload="auto"width="224" src="sm/assets/man_basket/Van-Gogh style portrait of a man spinning a basketball, oil painting, art by Van Gogh, 8k/pnp_per_frame_baseline_fps_30.mp4" autoplay loop controls muted/>
              </a>
            </td>
            <td style="text-align: center;">
              <a href="sm/assets/man_basket/Van-Gogh style portrait of a man spinning a basketball, oil painting, art by Van Gogh, 8k/result_fps30.mp4">
                <video preload="auto"width="224" src="sm/assets/man_basket/Van-Gogh style portrait of a man spinning a basketball, oil painting, art by Van Gogh, 8k/result_fps_30.mp4" autoplay loop controls muted/>
              </a>
            </td>
          </tr>
          <tr class="video-row">
            <td style="text-align: center;">
              <a href="videos/pca/tokens_origvideo_30.mp4">
                <video preload="auto"width="224" src="videos/pca/tokens_origvideo_30.mp4" autoplay loop controls muted/>
              </a>
            </td>
            <td style="text-align: center;">
              <a href="videos/pca/tokens_pnpvideo_30.mp4">
                <video preload="auto"width="224" src="videos/pca/tokens_pnpvideo_30.mp4" autoplay loop controls muted/>
              </a>
            </td>
            <td style="text-align: center;">
              <a href="videos/pca/tokens_flowvideo_30.mp4">
                <video preload="auto"width="224" src="videos/pca/tokens_flowvideo_30.mp4" autoplay loop controls muted/>
              </a>
            </td>
          </tr>
          <tr>
            
            <td colspan="3">
              <p style="margin-top: 20px; margin-bottom: -12px;">
                Our key finding is that a temporally-consistent edit can be achieved by enforcing consistency on the internal diffusion features across frames during the editing process.
                We achieve this by propagating a small set of edited features across frames, using the correspondences between the original video features.
                Given an input video I, we invert each frame, extract its tokens (i.e., output features from the self-attention modules), and extract inter-frame feature correspondences using a nearest-neighbor (NN) search. At each denoising step:
              </p>
              <ol>
                (I) We sample keyframes from the noisy video J_t and jointly edit them using an extended-attention block. The set of resulting edited tokens is T_base.</li>
                <br>
                (II) We propagate the edited tokens across the video according to the pre-computed correspondences of the original video features.</li>
              </ol>
              To denoise J_t, we feed each frame to the network and replace the generated tokens with the tokens obtained from the propagation step (II).
            </td>
          </tr>
        </tbody>
      </table> -->

        <div>
          <td colspan="3"><img src="static/images/pipline.png" alt="" width="1000" /></td>
        </div>
      <p>Overall review of CustomCrafter. For subject learning, we adopt LoRA to construct Spatial Subject Learning Module, which update the Query, Key, and Value parameters of attention layers in all Spatial Transformer models. In the process of generating videos, we divide the denoising process into two phases: the motion layout repair process and the subject appearance repair process. By reducing the influence of the Spatial Subject Learning Module in the motion layout repair process, and restoring it in the subject appearance repair process to repair the details of the subject. </p>
      </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!--End paper poster -->

<!-- video preload="auto"carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <!-- <div class="container is-max-desktop"> -->
      <div class="columns is-centered has-text-centered">
        <div class="column custom-width">
          <h2 class="title is-3">Comparison with SOTA</h2>
          <div class="content has-text-justified" align="center">
            <div id="results-carousel" class="carousel results-carousel" align="center">
              <div class="item item-video1">
                <video preload="auto"poster="" id="video1" autoplay controls muted loop width="1024px">
                  <!-- Your video preload="auto"file here -->
                  <source src="static/videos/web_demo_1.mp4"
                  type="video/mp4">
                </video>
              </div>
              <div class="item item-video2">
                <video preload="auto"poster="" id="video2" autoplay controls muted loop width="1024px">
                  <!-- Your video preload="auto"file here -->
                  <source src="static/videos/web_demo_2.mp4"
                  type="video/mp4">
                </video>
              </div>
              <div class="item item-video3">
                <video preload="auto"poster="" id="video3" autoplay controls muted loop width="1024px">
                  <!-- Your video preload="auto"file here -->
                  <source src="static/videos/web_demo_3.mp4"
                  type="video/mp4">
                </video>
              </div>
              <div class="item item-video4">
                <video preload="auto"poster="" id="video4" autoplay controls muted loop width="1024px">
                  <!-- Your video preload="auto"file here -->
                  <source src="static/videos/web_demo_4.mp4"
                  type="video/mp4">
                </video>
              </div>
              <div class="item item-video5">
                <video preload="auto"poster="" id="video5" autoplay controls muted loop width="1024px">
                  <!-- Your video preload="auto"file here -->
                  <source src="static/videos/web_demo_5.mp4"
                  type="video/mp4">
                </video>
              </div>
              <div class="item item-video6">
                <video preload="auto"poster="" id="video6" autoplay controls muted loop width="1024px">
                  <!-- Your video preload="auto"file here -->
                  <source src="static/videos/web_demo_6.mp4"
                  type="video/mp4">
                </video>
              </div>
            </div>
              
              

          </div>
    
        </div>
      </div>
  </div>
  <p align="center">Qualitative comparison of customized video generation with both subjects and motions. <br>Without guidance from additional videos, our method significantly outperforms in terms of concept combination.</p>
</section>
<!-- End video preload="auto"carousel -->



<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column custom-width">
          <h2 class="title is-3">References</h2>
          <div class="content has-text-justified">
            <p>
              <a name="customdiffusion" id="customdiffusion"></a>
              [1] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion CVPR, 2023.
            </p>
            <p>
              <a name="dreamvideo" id="dreamvideo"></a>
              [2] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos with customized subject and motion. CVPR, 2024.
            </p>
  
          </div>
        </div>
      </div>
    </div>  
  </div>  
</section>


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{wu2024customcrafter,
        title={CustomCrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities},
        author={Wu, Tao and Zhang, Yong and Wang, Xintao and Zhou, Xianpan and Zheng, Guangcong and Qi, Zhongang and Shan, Ying and Li, Xi},
        journal={arXiv preprint arXiv:2408.13239},
        year={2024}
      }
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
    <script>
      window.addEventListener('DOMContentLoaded', (event) => {
        const videoWrappers = document.querySelectorAll('.video-wrapper');
      
        videoWrappers.forEach(wrapper => {
          const defaultVideo = wrapper.querySelector('.default-video');
          const aspectRatio = defaultVideo.videoWidth / defaultVideo.videoHeight;
          const height = wrapper.offsetWidth / aspectRatio;
      
          wrapper.style.height = `${height}px`;
      
          wrapper.addEventListener('mouseenter', () => {
            defaultVideo.pause();
            hoverVideo.play();
          });
      
          wrapper.addEventListener('mouseleave', () => {
            defaultVideo.play();
            hoverVideo.pause();
          });
        });
      }); 
      $(document).ready(function() {
        var carouselItems = $('.carousel .item');
        var numItems = carouselItems.length;
        var numVideos = 5;
        var currentIndex = 0;
    
        $('.carousel').on('click', function() {
          currentIndex++;
          if (currentIndex + numVideos <= numItems) {
            carouselItems.removeClass('active');
            carouselItems.slice(currentIndex, currentIndex + numVideos).addClass('active');
          } else {
            currentIndex = 0;
            carouselItems.removeClass('active');
            carouselItems.slice(currentIndex, currentIndex + numVideos).addClass('active');
          }
        });
    
        carouselItems.slice(currentIndex, currentIndex + numVideos).addClass('active');
      });
    </script>
</body>
</html>
